{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOjY1JrNW8E4"
   },
   "source": [
    "# Model Training\n",
    "This notebook describes the construction, training and testing of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the [dataset](https://datashare.is.ed.ac.uk/handle/10283/2791)\n",
    "The files are enumerated and organized into noisy, clean pairs. The audio is loaded, resampled from 48 kHz to 16 kHz and regained such that the loudest sample has absolute value 1. Only a small random selection of the training dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def peak_amplitude(samples):\n",
    "    return max(abs(np.min(samples)), abs(np.max(samples)))\n",
    "\n",
    "TARGET_SR = 16000\n",
    "def load_audio(path, target_sr=TARGET_SR):\n",
    "    if target_sr == 48000:\n",
    "        y, sr = librosa.load(path)\n",
    "    else:\n",
    "        y, sr = librosa.load(path, sr=target_sr, res_type='kaiser_fast')\n",
    "    return y / peak_amplitude(y)\n",
    "\n",
    "def progressbar(it, prefix=\"\", size=100):\n",
    "    count = len(it)\n",
    "    def show(j):\n",
    "        x = int(size*j/count)\n",
    "        clear_output(wait=True)\n",
    "        print(\"%s[%s%s] %i/%i\\r\" % (prefix, \"#\"*x, \".\"*(size-x), j, count)) \n",
    "    show(0)\n",
    "    for j, item in enumerate(it):\n",
    "        yield item\n",
    "        show(j + 1)\n",
    "        \n",
    "def load_file_pairs(pairs):\n",
    "    result = []\n",
    "    for pair in progressbar(pairs): # A progressbar is shown for the lengthy loading process.\n",
    "        result.append(tuple(map(load_audio, pair)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_trainset_28spk_directory = \"DS_10283_2791/noisy_trainset_28spk_wav\"\n",
    "clean_trainset_28spk_directory = \"DS_10283_2791/clean_trainset_28spk_wav\"\n",
    "noisy_trainset_56spk_directory = \"DS_10283_2791/noisy_trainset_56spk_wav\"\n",
    "clean_trainset_56spk_directory = \"DS_10283_2791/clean_trainset_56spk_wav\"\n",
    "trainset_files = [(os.path.join(dirs[0], filename), os.path.join(dirs[1], filename))\n",
    "                  for dirs in [(noisy_trainset_28spk_directory, clean_trainset_28spk_directory),\n",
    "                                 (noisy_trainset_56spk_directory, clean_trainset_56spk_directory)]\n",
    "                  for filename in os.listdir(dirs[0])]\n",
    "\n",
    "TRAINSET_SIZE = 2000\n",
    "random.shuffle(trainset_files)\n",
    "trainset_files = trainset_files[:TRAINSET_SIZE]\n",
    "trainset_audio = load_file_pairs(trainset_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_testset_directory = \"DS_10283_2791/noisy_testset_wav\"\n",
    "clean_testset_directory = \"DS_10283_2791/clean_testset_wav\"\n",
    "test_files = [(os.path.join(noisy_testset_directory, filename),\n",
    "                  os.path.join(clean_testset_directory, filename))\n",
    "                 for filename in os.listdir(noisy_testset_directory)]\n",
    "\n",
    "test_audio = load_file_pairs(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the model\n",
    "We begin with a structure similar to a WaveNet. The main difference being that the padding of our dilated convolutional layers is non-causal. The sum of the skip-connections is then entered into a series of convolutional layers.  \n",
    "The input of the model will be a short slice of noisy audio (a subsequence of samples) and the output is meant to be a slice of the same length with denoised audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-Ay_XOXJjPY",
    "outputId": "5dee8e52-703e-4a86-c5a3-d6d0c92a245c"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, Flatten, Dense, \\\n",
    "  Input, Activation, Add, Multiply\n",
    "\n",
    "# This code served as inspiration:\n",
    "# https://github.com/usernaamee/keras-wavenet\n",
    "def wavenet_residual_block(filters, kernel_size, dilation_rate):\n",
    "    def f(input_):\n",
    "        conv = Conv1D(filters, kernel_size, \n",
    "                      dilation_rate=dilation_rate,\n",
    "                      padding='same')(input_)\n",
    "        tanh = Activation('tanh')(conv)\n",
    "        sigmoid = Activation('sigmoid')(conv)\n",
    "        merged = Multiply()([tanh, sigmoid])\n",
    "        out = Conv1D(1, 1, padding='same')(merged)\n",
    "        residual_out = Add()([out, input_])\n",
    "        skip_out = Activation('relu')(out)\n",
    "        return residual_out, skip_out\n",
    "    return f\n",
    "\n",
    "def wavenet_convolutional_layers(filters, kernel_size, depth):\n",
    "    def f(input_):\n",
    "        residual_out = input_\n",
    "        skip_connections = []\n",
    "        for i in range(1, depth+1):\n",
    "            dilation_rate = 2**(i % 9)\n",
    "            residual_out, skip_out = wavenet_residual_block(\n",
    "                filters, kernel_size, dilation_rate)(residual_out)\n",
    "            skip_connections.append(skip_out)\n",
    "        sum_ = Add()(skip_connections)\n",
    "        act = Activation('relu')(sum_)\n",
    "        return act\n",
    "    return f\n",
    "\n",
    "def wavenet(input_size):\n",
    "    input_ = Input(shape=(input_size, 1))\n",
    "    net = wavenet_convolutional_layers(128, 3, 30)(input_)\n",
    "    net = Conv1D(2048, 3, padding='same')(net)\n",
    "    net = Conv1D(256, 3, padding='same')(net)\n",
    "    net = Conv1D(1, 1, padding='same')(net)\n",
    "    model = Model(input_, net)\n",
    "    model.compile(loss='MAE', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "SLICE_SIZE = 2048\n",
    "model = wavenet(SLICE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "The training samples consist of pairs of noisy and clean audio slices. A generator is used to create these slices and organize them into mini-batches for the Keras API to consume. The placement of the slices within the audio files are random, as to avoid any bias caused by repeated slices. Setting a high slice density parameter allows more overlapping slices to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovh4wE1qiIug"
   },
   "outputs": [],
   "source": [
    "def batch_count(noisy_clean_audio_pairs, slice_size, batch_size, slice_density=1.0):\n",
    "    slices = sum(int(clean.shape[0] / slice_size * slice_density)\n",
    "                 for noisy, clean in noisy_clean_audio_pairs)\n",
    "    return slices // batch_size\n",
    "\n",
    "def slice_generator(noisy_clean_audio_pairs, slice_size, batch_size, slice_density=1.0):\n",
    "    # Memory optimalization: Instead of allocating a new numpy array for every batch,\n",
    "    # two alternating buffers are used. Having one buffer does not suffice, as \n",
    "    # the Keras API allows no modifications to arrays passed to it.\n",
    "    batch = (np.zeros((batch_size, slice_size)), np.zeros((batch_size, slice_size)))\n",
    "    next_batch = (np.zeros((batch_size, slice_size)), np.zeros((batch_size, slice_size)))\n",
    "    batch_x, batch_y = batch\n",
    "    while True:\n",
    "        i = 0\n",
    "        for noisy_audio, clean_audio in noisy_clean_audio_pairs:\n",
    "            slice_count = int(clean_audio.shape[0] / slice_size * slice_density)\n",
    "            for _ in range(slice_count):\n",
    "                j = np.random.randint(clean_audio.shape[0] - slice_size - 1)\n",
    "                batch_x[i, :] = noisy_audio[j:j + slice_size]\n",
    "                batch_y[i, :] = clean_audio[j:j + slice_size]\n",
    "                i += 1\n",
    "                if i == batch_size:\n",
    "                    yield batch\n",
    "                    batch, next_batch = next_batch, batch\n",
    "                    batch_x, batch_y = batch\n",
    "                    i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyY8JZibRuMI",
    "outputId": "6cca55f0-3d8c-49bb-c490-61a3a0491a95"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/model.hdf5',\n",
    "    save_freq='epoch',\n",
    "    save_best_only=True)\n",
    "\n",
    "rlrp = ReduceLROnPlateau(\n",
    "    min_delta=0.0005,\n",
    "    patience=3, \n",
    "    verbose=1,\n",
    "    monitor='val_loss')\n",
    "\n",
    "es = EarlyStopping(\n",
    "    patience=4, \n",
    "    verbose=1,\n",
    "    monitor='val_loss',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "SLICE_DENSITY = 1.0\n",
    "EPOCHS = 20\n",
    "VALIDATION_SPLIT = 0.4\n",
    "\n",
    "random.shuffle(trainset_audio)\n",
    "train_audio, val_audio = train_test_split(trainset_audio, test_size=VALIDATION_SPLIT)\n",
    "\n",
    "train_gen = slice_generator(train_audio, SLICE_SIZE, BATCH_SIZE, SLICE_DENSITY)\n",
    "train_batches = batch_count(train_audio, SLICE_SIZE, BATCH_SIZE, SLICE_DENSITY)\n",
    "\n",
    "val_gen = slice_generator(val_audio, SLICE_SIZE, BATCH_SIZE, SLICE_DENSITY)\n",
    "val_batches = batch_count(val_audio, SLICE_SIZE, BATCH_SIZE, SLICE_DENSITY)\n",
    "\n",
    "model.fit(x=train_gen,\n",
    "          steps_per_epoch=train_batches,\n",
    "          validation_data=val_gen,    \n",
    "          validation_steps=val_batches,\n",
    "          epochs=EPOCHS,\n",
    "          callbacks=[checkpoint, rlrp, es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "The loss is calculated for the entire testing dataset. Individual test files are evaluated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SLICE_DENSITY = 2.0\n",
    "test_gen = slice_generator(test_audio, SLICE_SIZE, BATCH_SIZE, TEST_SLICE_DENSITY)\n",
    "test_batches = batch_count(test_audio, SLICE_SIZE, BATCH_SIZE, TEST_SLICE_DENSITY)\n",
    "model.evaluate(x=test_gen, steps=test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import math \n",
    "\n",
    "def _model_predict(slice):\n",
    "    return np.array(model.predict(slice.reshape((1, SLICE_SIZE)))[0]).reshape((SLICE_SIZE))\n",
    "\n",
    "def _denoise_audio(audio, result):\n",
    "    slice_count = audio.shape[0] // SLICE_SIZE\n",
    "    for i in range(0, slice_count * SLICE_SIZE, SLICE_SIZE):\n",
    "        result[i:i+SLICE_SIZE] += _model_predict(audio[i:i+SLICE_SIZE])\n",
    "        \n",
    "def denoise_audio(audio):\n",
    "    # The input is rescaled in the same way as was the training data.\n",
    "    peak = peak_amplitude(audio)\n",
    "    npad = math.ceil(audio.shape[0] / SLICE_SIZE) * SLICE_SIZE\n",
    "    padded = np.pad(audio, (0, npad), 'reflect')\n",
    "    result = np.zeros(padded.shape)\n",
    "    _denoise_audio(padded / peak, result)\n",
    "    return result[:audio.shape[0]] * peak\n",
    "\n",
    "# Run the denoising for multiple rounds with varying time offsets, then average the results.\n",
    "# This is meant to eliminate random noise introduced by the model itself.\n",
    "def denoise_audio_multiple(audio, n=8):\n",
    "    peak = peak_amplitude(audio)\n",
    "    result = np.zeros((audio.shape[0] + SLICE_SIZE))\n",
    "    for offset in range(0, SLICE_SIZE - 1, SLICE_SIZE // n):\n",
    "        _denoise_audio(audio[offset:] / peak, result[offset:])\n",
    "    clipped = result[:audio.shape[0]]\n",
    "    average = clipped / math.ceil(SLICE_SIZE / (SLICE_SIZE // n))\n",
    "    return average  * peak\n",
    "\n",
    "def do_test(noisy_clean_path_pair):\n",
    "    tsr = TARGET_SR\n",
    "    noisy_res, _ = librosa.load(noisy_clean_path_pair[0], sr=tsr, res_type='kaiser_best')\n",
    "    result = denoise_audio(noisy_res)\n",
    "    noisy, sr = librosa.load(noisy_clean_path_pair[0])\n",
    "    clean, sr = librosa.load(noisy_clean_path_pair[1])\n",
    "    clean_res = librosa.resample(clean, sr, tsr, res_type='kaiser_best')[:noisy_res.shape[0]]\n",
    "    ipd.display(ipd.Audio(noisy_res, rate=tsr))\n",
    "    ipd.display(ipd.Audio(result, rate=tsr))\n",
    "    ipd.display(ipd.Audio(noisy_res - result, rate=tsr))\n",
    "    ipd.display(ipd.Audio(noisy_res - clean_res, rate=tsr))\n",
    "    \n",
    "do_test(test_files[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
